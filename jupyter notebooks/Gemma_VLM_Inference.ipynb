{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "- To do inference within the notebook:\n",
        "    1. Put all images in a directory.\n",
        "    2. In the last cell of the notebook, set the input_folder_vlm variable to the directory where the images are kept.        \n",
        "    3. Set the json_output_folder variable to a directory where the JSON files should be saved. This folder will be create automatically.\n",
        "    4. Set the VLM_BATCH_SIZE according to the available GPU memory. [24GB VRAM can handle batch size of upto 6]\n",
        "    5. Run all the cells in the notebook, the inference will be carried out in the final cell.\n",
        "        1. Ensure the YOLOv5 model has been downloaded. This should be automatically done when you run the code in the ‘Setup Environment’ section of the notebook. If it does not work, then follow the instructions on the notebook and download the YOLO model in the same directory as the notebook.\n",
        "    6. If you want to do inference with the base model (instead of the finetuned model), set the use_finetuned variable in the first cell to False.\n",
        "    7. To speed up inference, you can also disable Chain of Thought in the output by setting cot = False"
      ],
      "metadata": {
        "id": "xBdTLaJKV7Ls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Finetuned or Base Model\n",
        "Set the use_finetuned model to True to do infernece using the finetuned model. If it is set to False, inference will be done using the base model of Qwen."
      ],
      "metadata": {
        "id": "fLKd-CTnmCTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "use_finetuned = True\n",
        "cot = True"
      ],
      "metadata": {
        "id": "6-WqpNIsmG9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iyn587JD56FK"
      },
      "source": [
        "# Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjacVeS556FM"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERXyYeM456FN"
      },
      "outputs": [],
      "source": [
        "# !pip install git+https://github.com/huggingface/transformers accelerate qwen-vl-utils seaborn pyyaml ultralytics hf_xet peft json5 bitsandbytes einops safetensors\n",
        "# # !pip uninstall -q torch -y\n",
        "# # !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\n",
        "\n",
        "# #this will download 3 sample images and put them in a folder named \"input_folder\"\n",
        "# #the following commands will only work on a linux environment (such as colab)\n",
        "# #if you are running locally on windows, please manually put any sample images in a folder and name it 'input_images'\n",
        "# !gdown --id 17PI4UeX2tDR2YDbWsRdVKFCECR9eYQ4_ -O file.zip\n",
        "# !unzip file.zip -d input_images\n",
        "\n",
        "# #the following downloads a yolov5 model to detect vehicles\n",
        "# !gdown --id 1SrbA34-ptAA7Nm92TpCcHMn3lvwh-EKh"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the above code block doesn't work, you can download the YOLOv5 model from here: https://drive.google.com/file/d/1SrbA34-ptAA7Nm92TpCcHMn3lvwh-EKh/view?usp=drive_link"
      ],
      "metadata": {
        "id": "zt3Ie-xFnIRI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozKSr66d56FN"
      },
      "source": [
        "## JSON Parser\n",
        "\n",
        "JSON objects have a specific syntax that must be followed so that they can be accurately parsed. LLMs may not always output the accurate syntax. The following function is used to ensure the JSON output from the LLM has proper syntax. The function takes the LLM's output and outputs a proper JSON object. We call this function in a later part of the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JRozcwfFdUi"
      },
      "outputs": [],
      "source": [
        "def normalize_json5(s: str, img_name):\n",
        "    \"\"\"\n",
        "    Strip markdown fences and parse using json5 for lenient JSON parsing.\n",
        "    Returns the Python object on success or None on failure.\n",
        "    \"\"\"\n",
        "    # Remove any ``` or ```json code fences\n",
        "    s = re.sub(r\"```(?:json)?\\s*([\\s\\S]*?)```\", r\"\\1\", s).strip()\n",
        "    try:\n",
        "        return json5.loads(s)\n",
        "    except Exception as e:\n",
        "        print(f\"normalize_json5: failed to parse input as JSON5 for image: {img_name}.\") # MODIFIED\n",
        "        print(\"Error message:\", str(e))\n",
        "        # Optionally log the faulty string:\n",
        "        print(\"Faulty string content:\")\n",
        "        print(s)\n",
        "        # print(f\"Image: {img_name}\") # This was already here, good.\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVplGhLk674U"
      },
      "source": [
        "# GEMMA SETUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "nBshyp0p69d0",
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "from transformers import AutoProcessor, Gemma3ForConditionalGeneration\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "import re\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import json5\n",
        "import time\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from peft import PeftModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3rWN-wA56FN"
      },
      "source": [
        "### Model and Processor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjUwr57I56FN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "9b253e1ad7d04b4481ae09fb81799025",
            "67588cf4ed5b4df1b29ceb7f95d1c240",
            "09abd0c4da7f47d39d643344509fd692",
            "3d9f61939ace44e7a97dc949e60a3429",
            "49623c1be2144d3a8394e1117cabccfa",
            "e175c830c4954ccc9041d253953df4d7",
            "956c147fc2b0457b839706df6a70fd6e",
            "e562879afdb946808f03a1af40530254",
            "8131b88f3f79460c96904bf63758bf99",
            "aed6646ddc96416a958e0921b5551012",
            "8ed14c95a0c340078faa520e5e3d5f5d"
          ]
        },
        "outputId": "babce92b-1c15-4233-acbc-104ab27da6c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/2.82G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b253e1ad7d04b4481ae09fb81799025"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Base (unfine‑tuned) model\n",
        "#we are using a 4-bit quantized version of Gemma3 4B\n",
        "#the model id is found from the huggingface model page\n",
        "model_id = \"unsloth/gemma-3-4b-it-bnb-4bit\"\n",
        "\n",
        "#Downloading and loading the model on the GPU. This will load the original model\n",
        "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
        "    model_id, device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    ).to(\"cuda:0\").eval()\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "#Now we will load the LoRA adapters of a finetuned model. This adapters are attached\n",
        "#to the base model we loaded\n",
        "LORA_REPO = \"muqtasid87/gemma_fyp_lora_adapters\"\n",
        "if use_finetuned:\n",
        "  model = PeftModel.from_pretrained(\n",
        "      model,                     # pass base model\n",
        "      LORA_REPO,\n",
        "      is_trainable=False,\n",
        "      torch_dtype=torch.bfloat16,# inference only\n",
        "  ).to('cuda')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51UPrwvA7OFN"
      },
      "source": [
        "# System and User Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6U7w1_556FO"
      },
      "outputs": [],
      "source": [
        "if cot:\n",
        "  system_prompt = \"\"\"\n",
        "  You are a vehicle‑vision expert. Given one image of a single vehicle, you must:\n",
        "  1. Visual Observation & Chain of Thought (CoT): Systematically observe and describe all discernible visual features of the vehicle. Be highly detailed and specific,\n",
        "    focusing on attributes relevant to classification (e.g., body style, number of doors, roofline, ground clearance, wheel/axle count, cargo area, specialized equipment, visible markings).\n",
        "    Explain your step-by-step reasoning process for each classification decision, explicitly linking visual cues to the chosen categories and attributes. Compare all possible categories with the\n",
        "    visual cues.\n",
        "  2. Category Classification: Identify the most appropriate main Category from the Unified Vehicle Taxonomy provided in the user prompt.\n",
        "  3. Subcategory Classification: Identify the most appropriate fine-grained Subcategory under the chosen Category from the Unified Vehicle Taxonomy.\n",
        "    If no specific subcategory is visually discernible or applicable, output \"General\".\n",
        "  4. Attribute Extraction: Count visible wheels, infer the number of axles, and identify specific boolean attributes (Is_taxi, Is_school_bus, Is_emergency_vehicle, License_plate_visible).\n",
        "  5. Output ONLY a JSON object with only the given keys.\n",
        "\n",
        "  Do NOT output any other text.\n",
        "  \"\"\"\n",
        "else:\n",
        "  system_prompt = \"\"\"\n",
        "  You are a vehicle‑vision expert. Given one image of a single vehicle, you must:\n",
        "  1. Category Classification: Identify the most appropriate main Category from the Unified Vehicle Taxonomy provided in the user prompt.\n",
        "  2. Subcategory Classification: Identify the most appropriate fine-grained Subcategory under the chosen Category from the Unified Vehicle Taxonomy.\n",
        "    If no specific subcategory is visually discernible or applicable, output \"General\".\n",
        "  3. Attribute Extraction: Count visible wheels, infer the number of axles, and identify specific boolean attributes (Is_taxi, Is_school_bus, Is_emergency_vehicle, License_plate_visible).\n",
        "  4. Output ONLY a JSON object with only the given keys.\n",
        "\n",
        "  Do NOT output any other text.\n",
        "  \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFr3uyz956FO"
      },
      "outputs": [],
      "source": [
        "if cot:\n",
        "  user_prompt = \"\"\"\n",
        "  Analyze the vehicle in this image and output a structured JSON with fine-grained classification based on the provided Unified Vehicle Taxonomy and visual cues.\n",
        "  Use detailed reasoning steps in 'CoT' to explain your decision. If you are unable to confidently classify the vehicle based on the visual information,\n",
        "  set 'Category' and 'Subcategory' to 'Unclassified'.\n",
        "\n",
        "  **Unified Vehicle Taxonomy:**\n",
        "  1. Unclassified  - vehicle not confidently identifiable\n",
        "      • Used when the vehicle cannot be confidently classified into any defined category based on visual cues (e.g., due to poor image quality, occlusion, or ambiguity, or the vehicle not belonging to any of the given classes).\n",
        "  2. Car – standard passenger vehicles\n",
        "      • Sedan (Low-profile, elongated body with a distinct three-box silhouette: separate sections for engine, cabin, and trunk. Usually has four doors.)\n",
        "      • Hatchback (Compact body with a sloping rear roofline.)\n",
        "      • Coupe (Two-door layout with a short, sloped roof and a sporty, compact profile. Rear passenger space is visually smaller or limited.)\n",
        "      • Convertible (Vehicle with a visibly retractable or absent roof.)\n",
        "      • Sports (Aggressively styled, low-ground-clearance vehicle with wide stance, aerodynamic body lines, large wheels, and often two doors.)\n",
        "  3. SUV – taller chassis, off‑road capability\n",
        "      • Compact_SUV (Small, tall-bodied vehicle with short overhangs, high ground clearance, and rugged cladding. Compact proportions compared to full-size SUVs.)\n",
        "      • Mid‑Size_SUV (Moderate size with tall roofline, bold grille, and larger wheels. Typically appears more muscular than compact variants.)\n",
        "      • Full‑Size_SUV (Large and boxy with a high roofline, prominent grille, wide stance, and often roof rails. Rear often includes large vertical tail lights.)\n",
        "      • Crossover (Curved and car-like silhouette with a taller ride height. Smooth body lines and smaller gaps between wheels and body.)\n",
        "  4. MPV_Small – small multi‑purpose vans\n",
        "      • Compact_MPV (Tall, narrow-bodied vehicle with a short hood and vertical rear end. Often features sliding doors and large glass areas.)\n",
        "      • Minivan (Rounded, spacious-looking body with a long roofline and visibly large side windows. Typically has sliding rear doors.)\n",
        "\n",
        "  5. MPV_Big – large multi‑purpose vans\n",
        "      • Large_MPV (Bulky, tall profile with extended rear overhang and large windows. Wide stance with premium-looking body finishes.)\n",
        "      • 7‑Seater_MPV (Similar to Large_MPV but may show visual hints of interior layout such as larger rear side windows for third-row access.)\n",
        "  6. Pickup_Truck – open cargo bed\n",
        "      • Regular_Cab (Two-door truck with a small cabin and visibly long, open cargo bed. Clear separation between cab and bed.)\n",
        "      • Extended_Cab (Visibly longer cab than regular version, sometimes with small rear doors. Cargo bed is medium length.)\n",
        "      • Crew_Cab (Four full-size doors, full rear seating area, and slightly shorter bed. Balanced length between cabin and cargo area.)\n",
        "  7. Bus – passenger transport\n",
        "      • City_Bus (Long, boxy structure with wide doors (usually two or more), large windows, and low ground clearance.)\n",
        "      • Coach (Sleek and elongated body with high windows, luggage compartments beneath, and a streamlined roofline. long‑distance, luggage bays)\n",
        "      • School_Bus (Boxy, mid-length vehicle with a protruding front nose and high visibility features)\n",
        "  8. Motorcycle – two‑wheeled motor vehicles, slim and small silhoutte, usually single headlights\n",
        "      • Motorbike (standard bike)\n",
        "      • Scooter (step‑through frame)\n",
        "  9. Vans – enclosed cargo/passenger vans\n",
        "      • Cargo_Van (Boxy rear with few or no side windows in the cargo area. Minimal visual separation between cab and cargo body.)\n",
        "      • Passenger_Van (Boxy or slightly curved body with multiple side windows and visible seating rows through windows.)\n",
        "  10. Small_Truck – small 2‑axle commercial\n",
        "      • 2‑Axle_Small (Boxy or cab-over-engine design with visible dual rear wheels. 2 wheels per axles and compact cargo body.)\n",
        "      • 2-Axle_Big - (Larger than 2_Axle_Small, 4 rear wheels usually.)\n",
        "  11. Medium_Truck – medium 3‑axle commercial\n",
        "      • 3‑Axle_Medium (moderate cargo, Larger than 2-axle trucks, often with sleeper cab or larger cargo box. more robust body.)\n",
        "  12. Large_Truck – large 4‑axle commercial\n",
        "      • 4‑Axle_Large (heavy cargo, Very long frame with high ground clearance, extended wheelbase. Heavy-duty build with visible reinforced chassis.)\n",
        "  13. Heavy_Truck – heavy‑duty >5 axles\n",
        "      • 5+_Axle_Heavy (very heavy loads, Extra-long chassis with multiple connected trailer sections or components. highly segmented and reinforced appearance.)\n",
        "  14. Construction_and_Industrial – specialized machinery\n",
        "      • Bulldozer (Heavy tracked vehicle with a wide, flat metal blade at the front. Compact and powerful-looking body.)\n",
        "      • Excavator (Tracked or wheeled vehicle with a long articulated arm ending in a bucket. Rotating cab and counterweight at the rear.)\n",
        "      • Crane (Tall and narrow profile with a boom or arm extending upward or outward. Often mounted on a truck or mobile base.)\n",
        "      • Forklift (Small, compact industrial vehicle with upright forks at the front and a protective overhead cage.)\n",
        "      • Dump_Truck (Large, boxy bed with visible rear hinge and hydraulic lift system. High ground clearance and chunky tires.)\n",
        "      • Mixer (Truck with a large, rotating drum mounted on the rear. Drum is tilted and often striped or textured.)\n",
        "  15. Tanker – liquid cargo trailers\n",
        "      • Fuel_Tanker (Smooth, cylindrical tank mounted on a truck or trailer. Usually has visible valves and side or rear hatches.)\n",
        "      • Chemical_Tanker (Cylindrical tank with additional external piping, valves, and often hazmat signage.)\n",
        "      • Gas_Tanker (Rounded or bullet-shaped tank with extra reinforcement bands. May have insulated or pressurized appearance.)\n",
        "  16. Container – intermodal shipping boxes\n",
        "      • 20ft_Container (Short rectangular metal box with corner fittings. Often marked with codes or shipping logos. Proportions clearly shorter than 40ft variant.)\n",
        "      • 40ft_Container (Longer rectangular metal box with similar structure to 20ft but visually stretched. Usually spans most of a trailer.)\n",
        "  17. Trailer – unpowered cargo carriers\n",
        "      • Flatbed_Trailer (Open, flat platform with no walls or roof.)\n",
        "      • Car_Carrier (Double-decked or angled trailer frame with visible ramps and slots for securing vehicles.)\n",
        "      • Lowboy_Trailer (Trailer with a distinct drop in deck height between the gooseneck and rear wheels. Allows transport of tall equipment.)\n",
        "      • Refrigerator_Trailer (Boxy, fully enclosed trailer with smooth sides and a visible refrigeration unit (often front-mounted).)\n",
        "\n",
        "  **Output JSON keys & descriptors:**\n",
        "      1. **CoT**: Your step-by-step observations of all the details of the vehicle, explicitly linking visual cues to classification decisions. If the vehicle is unclassified, explain why classification was not possible (e.g., \"Image too blurry,\" \"Vehicle heavily obscured\").\n",
        "      2. **Category**: The main vehicle type, chosen from the **Unified Vehicle Taxonomy**. Output \"Unclassified\" if the vehicle cannot be confidently classified.\n",
        "      3. **Subcategory**: The fine-grained vehicle type, chosen from the **Unified Vehicle Taxonomy**. If not applicable or visually discernible, output \"General\". Output \"Unclassified\" if the vehicle cannot be confidently classified.\n",
        "      4. **Number_of_wheels_visible**: An integer count of all wheels clearly visible in the image. Output 0 if the vehicle is Unclassified or no wheels are visible.\n",
        "      5. **Number_of_axles_inferred_from_number_of_wheels**: An integer representing the inferred number of axles based on the visible wheels and vehicle type. For passenger vehicles, 2 wheels visible on one side typically implies 2 axle. For trucks and heavy vehicles, infer axles based on visible wheel sets. Consider dual wheels as a single wheel set for axle inference. Output 0 if the vehicle is Unclassified or axles cannot be inferred.\n",
        "      6. **Is_taxi**: A boolean (true/false). True if clear commercial taxi markings are visible (e.g., rooftop sign, specific livery, visible taxi meter, company logo). False otherwise.\n",
        "      7. **Is_school_bus**: A boolean (true/false). True if the vehicle is clearly identifiable as a school bus (e.g., distinct yellow color, \"SCHOOL BUS\" signage, stop sign arm, specific seating design). False otherwise.\n",
        "      8. **Is_emergency_vehicle**: A boolean (true/false). True if clear emergency vehicle markings are visible (e.g., siren, light bar, official police/ambulance/fire insignia, distinct emergency vehicle colors). False otherwise.\n",
        "      9. **License_plate_visible**: A boolean (true/false). True if a license plate is clearly visible and readable in the image. False otherwise.\n",
        "\n",
        "  JSON OUTPUT FORMAT EXAMPLE:\n",
        "\n",
        "      {\n",
        "        \"CoT\": \"...\",\n",
        "        \"Category\": \"...\",\n",
        "        \"Subcategory\": \"...\",\n",
        "        \"Number_of_wheels_visible\": 0,\n",
        "        \"Number_of_axles_inferred_from_number_of_wheels\": 0,\n",
        "        \"Is_taxi\": false,\n",
        "        \"Is_school_bus\": false,\n",
        "        \"Is_emergency_vehicle\": false,\n",
        "        \"License_plate_visible\": false\n",
        "      }\n",
        "\n",
        "  \"\"\"\n",
        "else:\n",
        "  user_prompt = \"\"\"\n",
        "  Analyze the vehicle in this image and output a structured JSON with fine-grained classification based on the provided Unified Vehicle Taxonomy and visual cues.\n",
        "  If you are unable to confidently classify the vehicle based on the visual information,\n",
        "  set 'Category' and 'Subcategory' to 'Unclassified'.\n",
        "\n",
        "  **Unified Vehicle Taxonomy:**\n",
        "  1. Unclassified  - vehicle not confidently identifiable\n",
        "      • Used when the vehicle cannot be confidently classified into any defined category based on visual cues (e.g., due to poor image quality, occlusion, or ambiguity, or the vehicle not belonging to any of the given classes).\n",
        "  2. Car – standard passenger vehicles\n",
        "      • Sedan (Low-profile, elongated body with a distinct three-box silhouette: separate sections for engine, cabin, and trunk. Usually has four doors.)\n",
        "      • Hatchback (Compact body with a sloping rear roofline.)\n",
        "      • Coupe (Two-door layout with a short, sloped roof and a sporty, compact profile. Rear passenger space is visually smaller or limited.)\n",
        "      • Convertible (Vehicle with a visibly retractable or absent roof.)\n",
        "      • Sports (Aggressively styled, low-ground-clearance vehicle with wide stance, aerodynamic body lines, large wheels, and often two doors.)\n",
        "  3. SUV – taller chassis, off‑road capability\n",
        "      • Compact_SUV (Small, tall-bodied vehicle with short overhangs, high ground clearance, and rugged cladding. Compact proportions compared to full-size SUVs.)\n",
        "      • Mid‑Size_SUV (Moderate size with tall roofline, bold grille, and larger wheels. Typically appears more muscular than compact variants.)\n",
        "      • Full‑Size_SUV (Large and boxy with a high roofline, prominent grille, wide stance, and often roof rails. Rear often includes large vertical tail lights.)\n",
        "      • Crossover (Curved and car-like silhouette with a taller ride height. Smooth body lines and smaller gaps between wheels and body.)\n",
        "  4. MPV_Small – small multi‑purpose vans\n",
        "      • Compact_MPV (Tall, narrow-bodied vehicle with a short hood and vertical rear end. Often features sliding doors and large glass areas.)\n",
        "      • Minivan (Rounded, spacious-looking body with a long roofline and visibly large side windows. Typically has sliding rear doors.)\n",
        "\n",
        "  5. MPV_Big – large multi‑purpose vans\n",
        "      • Large_MPV (Bulky, tall profile with extended rear overhang and large windows. Wide stance with premium-looking body finishes.)\n",
        "      • 7‑Seater_MPV (Similar to Large_MPV but may show visual hints of interior layout such as larger rear side windows for third-row access.)\n",
        "  6. Pickup_Truck – open cargo bed\n",
        "      • Regular_Cab (Two-door truck with a small cabin and visibly long, open cargo bed. Clear separation between cab and bed.)\n",
        "      • Extended_Cab (Visibly longer cab than regular version, sometimes with small rear doors. Cargo bed is medium length.)\n",
        "      • Crew_Cab (Four full-size doors, full rear seating area, and slightly shorter bed. Balanced length between cabin and cargo area.)\n",
        "  7. Bus – passenger transport\n",
        "      • City_Bus (Long, boxy structure with wide doors (usually two or more), large windows, and low ground clearance.)\n",
        "      • Coach (Sleek and elongated body with high windows, luggage compartments beneath, and a streamlined roofline. long‑distance, luggage bays)\n",
        "      • School_Bus (Boxy, mid-length vehicle with a protruding front nose and high visibility features)\n",
        "  8. Motorcycle – two‑wheeled motor vehicles, slim and small silhoutte, usually single headlights\n",
        "      • Motorbike (standard bike)\n",
        "      • Scooter (step‑through frame)\n",
        "  9. Vans – enclosed cargo/passenger vans\n",
        "      • Cargo_Van (Boxy rear with few or no side windows in the cargo area. Minimal visual separation between cab and cargo body.)\n",
        "      • Passenger_Van (Boxy or slightly curved body with multiple side windows and visible seating rows through windows.)\n",
        "  10. Small_Truck – small 2‑axle commercial\n",
        "      • 2‑Axle_Small (Boxy or cab-over-engine design with visible dual rear wheels. 2 wheels per axles and compact cargo body.)\n",
        "      • 2-Axle_Big - (Larger than 2_Axle_Small, 4 rear wheels usually.)\n",
        "  11. Medium_Truck – medium 3‑axle commercial\n",
        "      • 3‑Axle_Medium (moderate cargo, Larger than 2-axle trucks, often with sleeper cab or larger cargo box. more robust body.)\n",
        "  12. Large_Truck – large 4‑axle commercial\n",
        "      • 4‑Axle_Large (heavy cargo, Very long frame with high ground clearance, extended wheelbase. Heavy-duty build with visible reinforced chassis.)\n",
        "  13. Heavy_Truck – heavy‑duty >5 axles\n",
        "      • 5+_Axle_Heavy (very heavy loads, Extra-long chassis with multiple connected trailer sections or components. highly segmented and reinforced appearance.)\n",
        "  14. Construction_and_Industrial – specialized machinery\n",
        "      • Bulldozer (Heavy tracked vehicle with a wide, flat metal blade at the front. Compact and powerful-looking body.)\n",
        "      • Excavator (Tracked or wheeled vehicle with a long articulated arm ending in a bucket. Rotating cab and counterweight at the rear.)\n",
        "      • Crane (Tall and narrow profile with a boom or arm extending upward or outward. Often mounted on a truck or mobile base.)\n",
        "      • Forklift (Small, compact industrial vehicle with upright forks at the front and a protective overhead cage.)\n",
        "      • Dump_Truck (Large, boxy bed with visible rear hinge and hydraulic lift system. High ground clearance and chunky tires.)\n",
        "      • Mixer (Truck with a large, rotating drum mounted on the rear. Drum is tilted and often striped or textured.)\n",
        "  15. Tanker – liquid cargo trailers\n",
        "      • Fuel_Tanker (Smooth, cylindrical tank mounted on a truck or trailer. Usually has visible valves and side or rear hatches.)\n",
        "      • Chemical_Tanker (Cylindrical tank with additional external piping, valves, and often hazmat signage.)\n",
        "      • Gas_Tanker (Rounded or bullet-shaped tank with extra reinforcement bands. May have insulated or pressurized appearance.)\n",
        "  16. Container – intermodal shipping boxes\n",
        "      • 20ft_Container (Short rectangular metal box with corner fittings. Often marked with codes or shipping logos. Proportions clearly shorter than 40ft variant.)\n",
        "      • 40ft_Container (Longer rectangular metal box with similar structure to 20ft but visually stretched. Usually spans most of a trailer.)\n",
        "  17. Trailer – unpowered cargo carriers\n",
        "      • Flatbed_Trailer (Open, flat platform with no walls or roof.)\n",
        "      • Car_Carrier (Double-decked or angled trailer frame with visible ramps and slots for securing vehicles.)\n",
        "      • Lowboy_Trailer (Trailer with a distinct drop in deck height between the gooseneck and rear wheels. Allows transport of tall equipment.)\n",
        "      • Refrigerator_Trailer (Boxy, fully enclosed trailer with smooth sides and a visible refrigeration unit (often front-mounted).)\n",
        "\n",
        "  **Output JSON keys & descriptors:**\n",
        "      1. **Category**: The main vehicle type, chosen from the **Unified Vehicle Taxonomy**. Output \"Unclassified\" if the vehicle cannot be confidently classified.\n",
        "      2. **Subcategory**: The fine-grained vehicle type, chosen from the **Unified Vehicle Taxonomy**. If not applicable or visually discernible, output \"General\". Output \"Unclassified\" if the vehicle cannot be confidently classified.\n",
        "      3. **Number_of_wheels_visible**: An integer count of all wheels clearly visible in the image. Output 0 if the vehicle is Unclassified or no wheels are visible.\n",
        "      4. **Number_of_axles_inferred_from_number_of_wheels**: An integer representing the inferred number of axles based on the visible wheels and vehicle type. For passenger vehicles, 2 wheels visible on one side typically implies 2 axle. For trucks and heavy vehicles, infer axles based on visible wheel sets. Consider dual wheels as a single wheel set for axle inference. Output 0 if the vehicle is Unclassified or axles cannot be inferred.\n",
        "      5. **Is_taxi**: A boolean (true/false). True if clear commercial taxi markings are visible (e.g., rooftop sign, specific livery, visible taxi meter, company logo). False otherwise.\n",
        "      6. **Is_school_bus**: A boolean (true/false). True if the vehicle is clearly identifiable as a school bus (e.g., distinct yellow color, \"SCHOOL BUS\" signage, stop sign arm, specific seating design). False otherwise.\n",
        "      7. **Is_emergency_vehicle**: A boolean (true/false). True if clear emergency vehicle markings are visible (e.g., siren, light bar, official police/ambulance/fire insignia, distinct emergency vehicle colors). False otherwise.\n",
        "      8. **License_plate_visible**: A boolean (true/false). True if a license plate is clearly visible and readable in the image. False otherwise.\n",
        "\n",
        "  JSON OUTPUT FORMAT EXAMPLE:\n",
        "\n",
        "      {\n",
        "        \"Category\": \"...\",\n",
        "        \"Subcategory\": \"...\",\n",
        "        \"Number_of_wheels_visible\": 0,\n",
        "        \"Number_of_axles_inferred_from_number_of_wheels\": 0,\n",
        "        \"Is_taxi\": false,\n",
        "        \"Is_school_bus\": false,\n",
        "        \"Is_emergency_vehicle\": false,\n",
        "        \"License_plate_visible\": false\n",
        "      }\n",
        "\n",
        "  \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EytZBZL56FO"
      },
      "source": [
        "# YOLO Detector Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOgFL1D256FO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9ccffd6-5b15-4507-85bf-65e4a47eeb2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import uuid\n",
        "from ultralytics import YOLO\n",
        "import base64\n",
        "\n",
        "#path to downloaded YOLOv5 model\n",
        "model_path = r'best_avc_v5.pt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMQgms1S56FO"
      },
      "outputs": [],
      "source": [
        "def detect_vehicles_yolo_v5(image, model, conf_threshold=0.1):\n",
        "    \"\"\"\n",
        "    Detect vehicles in an image using YOLOv5 model.\n",
        "\n",
        "    Args:\n",
        "        image_filename (str): Name of the image file (for reference)\n",
        "        image (numpy.ndarray): Input image\n",
        "        model_path (str): Path to YOLOv5 model weights\n",
        "        conf_threshold (float): Confidence threshold for detections\n",
        "\n",
        "    Returns:\n",
        "        list: List of dictionaries containing detection information\n",
        "    \"\"\"\n",
        "\n",
        "    # Custom class names mapping (adjust based on your model)\n",
        "    class_names = {\n",
        "        1: 'class1_lightVehicle',\n",
        "        2: 'class2_mediumVehicle',\n",
        "        3: 'class3_heavyVehicle',\n",
        "        4: 'class4_taxi',\n",
        "        5: 'class5_bus',\n",
        "        6: 'class_motocycle'\n",
        "    }\n",
        "\n",
        "    # Vehicle classes\n",
        "    vehicle_classes = [1, 2, 3, 4, 5, 6]  # Adjust based on your model's class IDs\n",
        "\n",
        "    # Get image dimensions\n",
        "    img_height, img_width = image.shape[:2]\n",
        "\n",
        "    # Initialize list to store vehicle detections\n",
        "    vehicle_detections = []\n",
        "\n",
        "    try:\n",
        "        # Run inference\n",
        "        results = model(image)\n",
        "\n",
        "        # Parse results\n",
        "        detections = results.pandas().xyxy[0]  # Get detections as pandas DataFrame\n",
        "\n",
        "        for _, detection in detections.iterrows():\n",
        "            class_id = int(detection['class'])\n",
        "            confidence = float(detection['confidence'])\n",
        "\n",
        "            # Skip if not a vehicle class or below confidence threshold\n",
        "            if class_id not in vehicle_classes or confidence < conf_threshold:\n",
        "                continue\n",
        "\n",
        "            # Get bounding box coordinates\n",
        "            x1 = int(detection['xmin'])\n",
        "            y1 = int(detection['ymin'])\n",
        "            x2 = int(detection['xmax'])\n",
        "            y2 = int(detection['ymax'])\n",
        "\n",
        "            # Ensure coordinates are within image bounds\n",
        "            x1 = max(0, x1)\n",
        "            y1 = max(0, y1)\n",
        "            x2 = min(img_width, x2)\n",
        "            y2 = min(img_height, y2)\n",
        "\n",
        "            # Skip if bounding box is invalid\n",
        "            if x2 <= x1 or y2 <= y1:\n",
        "                continue\n",
        "\n",
        "            # Get class name\n",
        "            class_name = class_names.get(class_id, f\"class_{class_id}\")\n",
        "\n",
        "            # Create a unique ID for this detection\n",
        "            detection_id = str(uuid.uuid4())\n",
        "\n",
        "            # Crop the vehicle from the image\n",
        "            vehicle_crop = image[y1:y2, x1:x2]\n",
        "\n",
        "            # Skip if crop is empty\n",
        "            if vehicle_crop.size == 0:\n",
        "                continue\n",
        "\n",
        "            # Convert crop to base64\n",
        "            base64_crop = numpy_to_base64(vehicle_crop)\n",
        "\n",
        "            # Store detection info\n",
        "            detection = {\n",
        "                \"detection_id\": detection_id,\n",
        "                \"bounding_box\": {\n",
        "                    \"x1\": x1,\n",
        "                    \"y1\": y1,\n",
        "                    \"x2\": x2,\n",
        "                    \"y2\": y2,\n",
        "                    \"width\": x2 - x1,\n",
        "                    \"height\": y2 - y1\n",
        "                },\n",
        "                \"class\": class_name,\n",
        "                \"class_id\": class_id,\n",
        "                \"confidence\": confidence,\n",
        "                \"crop_base64\": base64_crop\n",
        "            }\n",
        "\n",
        "            vehicle_detections.append(detection)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during inference: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "    return vehicle_detections\n",
        "\n",
        "#each cropped region is saved as a numpy array\n",
        "#so we need to convert to base64 to pass into VLM\n",
        "def numpy_to_base64(image):\n",
        "    \"\"\"\n",
        "    Convert a numpy array image to a base64 encoded string.\n",
        "\n",
        "    Args:\n",
        "        image (numpy.ndarray): Image as numpy array\n",
        "\n",
        "    Returns:\n",
        "        str: Base64 encoded string\n",
        "    \"\"\"\n",
        "    # Encode the numpy array to jpg format\n",
        "    success, encoded_img = cv2.imencode('.jpg', image)\n",
        "    if not success:\n",
        "        return None\n",
        "\n",
        "    # Convert the binary data to base64 string\n",
        "    base64_data = base64.b64encode(encoded_img.tobytes()).decode('utf-8')\n",
        "\n",
        "    # Format as data URL\n",
        "    return f\"data:image/jpeg;base64,{base64_data}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ehg7Yc5z56FP"
      },
      "source": [
        "Function to combine YOLO regions with VLM inferences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAX95SkP56FP"
      },
      "outputs": [],
      "source": [
        "# function to combine detection region with the inference from the VLM and create a JSON file for every image\n",
        "def create_image_json(image_path, image, detections):\n",
        "    \"\"\"\n",
        "    Create a JSON object for an image with all its detections.\n",
        "\n",
        "    Args:\n",
        "        image_path (Path): Path to the image file\n",
        "        image (numpy.ndarray): The image array\n",
        "        detections (list): List of detection dictionaries with inference results\n",
        "\n",
        "    Returns:\n",
        "        dict: Image results as a JSON-serializable dictionary\n",
        "    \"\"\"\n",
        "    # Create the image results dictionary\n",
        "    image_results = {\n",
        "        \"image_filename\": image_path.name,\n",
        "        \"image_path\": str(image_path),\n",
        "        \"image_dimensions\": {\n",
        "            \"width\": image.shape[1],\n",
        "            \"height\": image.shape[0],\n",
        "            \"channels\": image.shape[2]\n",
        "        },\n",
        "        \"detection_count\": len(detections),\n",
        "        \"detections\": detections,\n",
        "    }\n",
        "\n",
        "    return image_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRgdHqwJ56FP"
      },
      "source": [
        "# VLM Inference Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NZTFZT856FP"
      },
      "source": [
        "### Base64 to PIL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwntOeIk56FP"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "\n",
        "# Helper function to convert base64 string to PIL Image\n",
        "def base64_to_pil(base64_str):\n",
        "    # Remove the \"data:image/jpeg;base64,\" part if present\n",
        "    if \"base64,\" in base64_str:\n",
        "        base64_str = base64_str.split(\"base64,\")[1]\n",
        "    try:\n",
        "        image_data = base64.b64decode(base64_str)\n",
        "        image = Image.open(BytesIO(image_data))\n",
        "        return image\n",
        "    except Exception as e:\n",
        "        print(f\"Error decoding base64 string or opening image: {e}\")\n",
        "        # print(f\"Problematic base64 string (first 100 chars): {base64_str[:100]}\") # For debugging\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVlZ50Yj56FP"
      },
      "source": [
        "### Batched VLM Inference Function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vlm_fine_grained_inference_batched(\n",
        "    imgs_base64_list,\n",
        "    system_prompt_text=system_prompt,\n",
        "    user_prompt_text=user_prompt,\n",
        "    img_names_list=None # Added for error reporting\n",
        "):\n",
        "    batch_size = len(imgs_base64_list)\n",
        "    if batch_size == 0:\n",
        "        return []\n",
        "\n",
        "    if img_names_list is None:\n",
        "        # Create dummy names if not provided, for consistent error reporting\n",
        "        img_names_list = [f\"image_in_batch_{i}\" for i in range(batch_size)]\n",
        "    elif len(img_names_list) != batch_size:\n",
        "        # Fallback if img_names_list is provided but has incorrect length\n",
        "        print(\"Warning: img_names_list length mismatch. Using generic names for error reporting.\")\n",
        "        img_names_list = [f\"image_in_batch_{i}\" for i in range(batch_size)]\n",
        "\n",
        "\n",
        "    batch_of_conversations = []\n",
        "\n",
        "    for i, img_b64 in enumerate(imgs_base64_list):\n",
        "        pil_image = base64_to_pil(img_b64)\n",
        "        current_img_name = img_names_list[i] if img_names_list and i < len(img_names_list) else f\"unknown_image_at_batch_idx_{i}\"\n",
        "\n",
        "        if pil_image is None:\n",
        "            print(f\"Warning: Failed to convert base64 image to PIL for {current_img_name}. Skipping its VLM processing.\")\n",
        "            current_item_messages = [\n",
        "                {\"role\": \"system\", \"content\": f\"Error: Image {current_img_name} could not be processed.\"},\n",
        "                {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": f\"Error in image {current_img_name}.\"}]}\n",
        "            ]\n",
        "        else:\n",
        "            current_item_messages = [\n",
        "            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_prompt}]},\n",
        "            {\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": img_b64}, {\"type\": \"text\", \"text\": user_prompt}]}\n",
        "        ]\n",
        "        batch_of_conversations.append(current_item_messages)\n",
        "\n",
        "    inputs = processor.apply_chat_template(\n",
        "        batch_of_conversations,\n",
        "        add_generation_prompt=True, tokenize=True, return_dict=True,\n",
        "        return_tensors=\"pt\", padding=True, truncation=True\n",
        "    ).to(model.device, dtype=torch.bfloat16) # Ensure dtype matches\n",
        "\n",
        "    input_len = inputs[\"input_ids\"].shape[-1]\n",
        "    decoded_batch_results = []\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        generations = model.generate(**inputs, max_new_tokens=512, do_sample=False)\n",
        "\n",
        "    for i in range(generations.shape[0]):\n",
        "        generation_item_tokens = generations[i][input_len:]\n",
        "        decoded_text = processor.decode(generation_item_tokens, skip_special_tokens=True)\n",
        "        decoded_batch_results.append(normalize_json5(decoded_text, img_names_list[i]))\n",
        "\n",
        "    return decoded_batch_results"
      ],
      "metadata": {
        "id": "kILUHGWoohYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNVneRMD8MrR"
      },
      "source": [
        "# Process Images (Putting detection and VLM inference together)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqFvmZb556FP"
      },
      "outputs": [],
      "source": [
        "def process_images_true_batching(input_folder, output_folder, model=model_path, conf_threshold=0.1, vlm_batch_size=2):\n",
        "\n",
        "    # Load YOLOv5 model\n",
        "    try:\n",
        "        # Try with force_reload first\n",
        "        model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_path, force_reload=True)\n",
        "        model.conf = conf_threshold\n",
        "    except Exception as e1:\n",
        "        try:\n",
        "            # Alternative: load directly with torch.load if custom model\n",
        "            model = torch.load(model_path, map_location='cpu')\n",
        "            if hasattr(model, 'conf'):\n",
        "                model.conf = conf_threshold\n",
        "        except Exception as e2:\n",
        "            print(f\"Error loading model with torch.hub: {str(e1)}\")\n",
        "            print(f\"Error loading model with torch.load: {str(e2)}\")\n",
        "            return []\n",
        "\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    image_exts = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff')\n",
        "    all_input_files = [p for ext in image_exts for p in Path(input_folder).glob(f'*{ext}')]\n",
        "    print(f\"Found {len(all_input_files)} images in '{input_folder}'\")\n",
        "    start_all_time = time.perf_counter()\n",
        "\n",
        "    all_images_data = {} # Stores {'img_path_str': {'img_cv2': img_obj_or_None, 'yolo_detections_with_vlm': []}}\n",
        "\n",
        "\n",
        "    # Stores items for VLM batching:\n",
        "    # [{'crop_b64': b64_str, 'img_name_for_vlm': \"original_image_name_crop_0\", 'image_path_str': img_path, 'original_det_idx': det_idx, 'yolo_det_obj': yolo_det_dict_without_crop}]\n",
        "    crops_to_vlm_batch = []\n",
        "\n",
        "    processed_image_count = 0\n",
        "    total_yolo_detections_count = 0\n",
        "    total_vlm_inferences_done = 0\n",
        "\n",
        "    # --- Stage 1: Iterate images, run YOLO, collect crops ---\n",
        "    for img_path in tqdm(all_input_files, desc=\"YOLO & Crop Collection\", unit=\"img\"):\n",
        "        img_path_str = str(img_path)\n",
        "        img_name = img_path.name # For VLM error reporting\n",
        "\n",
        "        img = cv2.imread(img_path_str)\n",
        "        if img is None:\n",
        "            tqdm.write(f\"[SKIP] Could not read '{img_name}'\")\n",
        "            all_images_data[img_path_str] = {'img_cv2': None, 'yolo_detections_with_vlm': []}\n",
        "            continue\n",
        "\n",
        "        all_images_data[img_path_str] = {'img_cv2': img, 'yolo_detections_with_vlm': []}\n",
        "\n",
        "        yolo_detections_current_image = detect_vehicles_yolo_v5(img, model, conf_threshold=0.1)\n",
        "        total_yolo_detections_count += len(yolo_detections_current_image)\n",
        "\n",
        "        for det_idx, yolo_det_obj in enumerate(yolo_detections_current_image):\n",
        "            crop_b64 = yolo_det_obj.pop(\"crop_base64\", None)\n",
        "            if crop_b64:\n",
        "                crops_to_vlm_batch.append({\n",
        "                    'crop_b64': crop_b64,\n",
        "                    'img_name_for_vlm': f\"{img_name}_crop_{det_idx}\", # Specific name for this crop\n",
        "                    'image_path_str': img_path_str,\n",
        "                    'original_det_idx': det_idx,\n",
        "                    'yolo_det_obj': yolo_det_obj\n",
        "                })\n",
        "\n",
        "        # --- Stage 2: Perform VLM inference in batches ---\n",
        "        if len(crops_to_vlm_batch) >= vlm_batch_size:\n",
        "            tqdm.write(f\"Processing VLM batch of {len(crops_to_vlm_batch)} crops...\")\n",
        "\n",
        "            b64_list_for_vlm = [item['crop_b64'] for item in crops_to_vlm_batch]\n",
        "            # Pass the specific names for VLM error reporting\n",
        "            img_names_for_vlm_batch = [item['img_name_for_vlm'] for item in crops_to_vlm_batch]\n",
        "            vlm_results = vlm_fine_grained_inference_batched(b64_list_for_vlm, img_names_list=img_names_for_vlm_batch)\n",
        "            total_vlm_inferences_done += len(vlm_results)\n",
        "\n",
        "            for i, vlm_output in enumerate(vlm_results):\n",
        "                item_info = crops_to_vlm_batch[i]\n",
        "                item_info['yolo_det_obj']['vlm_fine_grained_inference'] = vlm_output\n",
        "                all_images_data[item_info['image_path_str']]['yolo_detections_with_vlm'].append(item_info['yolo_det_obj'])\n",
        "\n",
        "            crops_to_vlm_batch = []\n",
        "\n",
        "    # Process any remaining crops\n",
        "    if crops_to_vlm_batch:\n",
        "        tqdm.write(f\"Processing final VLM batch of {len(crops_to_vlm_batch)} crops...\")\n",
        "        b64_list_for_vlm = [item['crop_b64'] for item in crops_to_vlm_batch]\n",
        "        img_names_for_vlm_batch = [item['img_name_for_vlm'] for item in crops_to_vlm_batch]\n",
        "        vlm_results = vlm_fine_grained_inference_batched(b64_list_for_vlm, img_names_list=img_names_for_vlm_batch)\n",
        "        total_vlm_inferences_done += len(vlm_results)\n",
        "\n",
        "        for i, vlm_output in enumerate(vlm_results):\n",
        "            item_info = crops_to_vlm_batch[i]\n",
        "            item_info['yolo_det_obj']['vlm_fine_grained_inference'] = vlm_output\n",
        "            all_images_data[item_info['image_path_str']]['yolo_detections_with_vlm'].append(item_info['yolo_det_obj'])\n",
        "        crops_to_vlm_batch = []\n",
        "\n",
        "    # --- Stage 3: Create JSON output files ---\n",
        "    for img_path_str, data in tqdm(all_images_data.items(), desc=\"Creating JSON files\", unit=\"file\"):\n",
        "        img_p = Path(img_path_str)\n",
        "        image_cv2_obj = data['img_cv2']\n",
        "\n",
        "        if image_cv2_obj is None:\n",
        "            img_json_data = create_image_json(img_p, np.zeros((100,100,3), dtype=np.uint8), [])\n",
        "        else:\n",
        "            img_json_data = create_image_json(img_p, image_cv2_obj, data['yolo_detections_with_vlm'])\n",
        "\n",
        "        out_file = Path(output_folder) / f\"{img_p.stem}_detections.json\"\n",
        "        with open(out_file, 'w') as f:\n",
        "            json.dump(img_json_data, f, indent=2)\n",
        "        processed_image_count += 1\n",
        "\n",
        "    end_all_time = time.perf_counter()\n",
        "    total_duration = end_all_time - start_all_time\n",
        "    avg_time = total_duration / len(all_input_files) if all_input_files else 0\n",
        "\n",
        "    print(f\"\\n--- Processing Summary ---\")\n",
        "    print(f\"Total images processed: {processed_image_count} / {len(all_input_files)}\")\n",
        "    print(f\"Total YOLO detections found: {total_yolo_detections_count}\")\n",
        "    print(f\"Total VLM inferences performed: {total_vlm_inferences_done}\")\n",
        "    print(f\"Total pipeline time: {total_duration:.2f} seconds\")\n",
        "    print(f\"Average time per image: {avg_time:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "jxNd7PKCoNUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_folder_vlm = \"input_images/images\"\n",
        "json_output_folder = f\"output_json\"\n",
        "\n",
        "VLM_BATCH_SIZE = 2\n",
        "process_images_true_batching(input_folder_vlm, json_output_folder, vlm_batch_size=VLM_BATCH_SIZE)"
      ],
      "metadata": {
        "id": "D2XZnEUvoOuO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67b916d5-d36a-4eb4-cf9b-182e1af11b9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2025-8-6 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7039792 parameters, 0 gradients, 15.8 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3 images in 'input_images/images'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLO & Crop Collection:   0%|          | 0/3 [00:00<?, ?img/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing VLM batch of 3 crops...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "YOLO & Crop Collection:  67%|██████▋   | 2/3 [03:23<01:41, 101.93s/img]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing VLM batch of 2 crops...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLO & Crop Collection: 100%|██████████| 3/3 [05:21<00:00, 107.03s/img]\n",
            "Creating JSON files: 100%|██████████| 3/3 [00:00<00:00, 2445.66file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Processing Summary ---\n",
            "Total images processed: 3 / 3\n",
            "Total YOLO detections found: 5\n",
            "Total VLM inferences performed: 5\n",
            "Total pipeline time: 321.09 seconds\n",
            "Average time per image: 107.03 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6iSt1t2iNf_I"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9b253e1ad7d04b4481ae09fb81799025": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_67588cf4ed5b4df1b29ceb7f95d1c240",
              "IPY_MODEL_09abd0c4da7f47d39d643344509fd692",
              "IPY_MODEL_3d9f61939ace44e7a97dc949e60a3429"
            ],
            "layout": "IPY_MODEL_49623c1be2144d3a8394e1117cabccfa"
          }
        },
        "67588cf4ed5b4df1b29ceb7f95d1c240": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e175c830c4954ccc9041d253953df4d7",
            "placeholder": "​",
            "style": "IPY_MODEL_956c147fc2b0457b839706df6a70fd6e",
            "value": "adapter_model.safetensors: 100%"
          }
        },
        "09abd0c4da7f47d39d643344509fd692": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e562879afdb946808f03a1af40530254",
            "max": 2816262456,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8131b88f3f79460c96904bf63758bf99",
            "value": 2816262456
          }
        },
        "3d9f61939ace44e7a97dc949e60a3429": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aed6646ddc96416a958e0921b5551012",
            "placeholder": "​",
            "style": "IPY_MODEL_8ed14c95a0c340078faa520e5e3d5f5d",
            "value": " 2.82G/2.82G [01:02&lt;00:00, 30.6MB/s]"
          }
        },
        "49623c1be2144d3a8394e1117cabccfa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e175c830c4954ccc9041d253953df4d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "956c147fc2b0457b839706df6a70fd6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e562879afdb946808f03a1af40530254": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8131b88f3f79460c96904bf63758bf99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aed6646ddc96416a958e0921b5551012": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ed14c95a0c340078faa520e5e3d5f5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}